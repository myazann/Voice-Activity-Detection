{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "VAD_Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myazann/Voice-Activity-Detection/blob/main/VAD_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e82XvUE7Rrql"
      },
      "source": [
        "## Connect to Google Drive and install required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUfJhCSNlKz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9df157db-0a2a-4a38-ff4e-675a7168cea4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHMncMVok0Lv"
      },
      "source": [
        "!pip install torchaudio\n",
        "!pip install pydub\n",
        "\n",
        "from torch.utils import data\n",
        "import torchaudio\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "import numpy as np\n",
        "import pydub\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import *\n",
        "from torch.optim import *\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "from sklearn.metrics import classification_report\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EHF_1b-R3cI"
      },
      "source": [
        "##Get the data from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYf1IIXilpa5"
      },
      "source": [
        "os.chdir(\"/content/drive/My Drive\")\n",
        "\n",
        "!cp Audio_Pad.zip /content\n",
        "!cp Eklenti.zip /content\n",
        "\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "!unzip Audio_Pad.zip\n",
        "!unzip Eklenti.zip\n",
        "\n",
        "!mv -v Eklenti/Train/Speech/* Audio_Pad/Train/Speech\n",
        "!mv -v Eklenti/Train/Non_Speech/* Audio_Pad/Train/Non_Speech\n",
        "!mv -v Eklenti/Val/Speech/* Audio_Pad/Val/Speech\n",
        "!mv -v Eklenti/Val/Non_Speech/* Audio_Pad/Val/Non_Speech\n",
        "\n",
        "!rm -rf Audio_Pad.zip\n",
        "!rm -rf Eklenti.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XedBo1JR9l2"
      },
      "source": [
        "## Create CustomDataset class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtwV4Jh2dW0L"
      },
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data, label):\n",
        "        self.data = torch.tensor(data, dtype=torch.float)\n",
        "        self.label = torch.tensor(label, dtype=torch.long)\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        item_data = self.data[index]\n",
        "        item_label = self.label[index]\n",
        "\n",
        "        return item_data, item_label\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfjXX_4WSY5H"
      },
      "source": [
        "##Create Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eCs_3KAkW2P"
      },
      "source": [
        "class BidirectionalGRU(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = nn.GRU(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SoundDetectorModel(Module):   \n",
        "    def __init__(self):\n",
        "        super(SoundDetectorModel, self).__init__()\n",
        "          \n",
        "        self.sound_detector_model =  Sequential(\n",
        "            self.ConvBlock(1, 32, 3, 1, 1),\n",
        "            self.ConvBlock(32, 32, 3, 1, 1, True),\n",
        "            self.ConvBlock(32, 64, 3, 1, 1),\n",
        "            self.ConvBlock(64, 64, 3, 1, 1, True),\n",
        "            self.ConvBlock(64, 128, 3, 1, 1, True),\n",
        "            Dropout(0.5),\n",
        "            self.ConvBlock(128, 256, 3, 1, 1),\n",
        "            self.ConvBlock(256, 256, 3, 1, 1, True),\n",
        "            Dropout(0.5),\n",
        "            self.ConvBlock(256, 512, 3, 1, 1),\n",
        "            self.ConvBlock(512, 1024, 3, 1, 1, True),\n",
        "            Dropout(0.5)\n",
        "            )\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            BidirectionalGRU(1024, 1024, 0.25, True),\n",
        "            Flatten(),\n",
        "            self.LinearBlock(32768, 4096),\n",
        "            self.LinearBlock(4096, 2048),\n",
        "            self.LinearBlock(2048, 256),\n",
        "\n",
        "            Linear(256, 2)\n",
        "        )\n",
        "\n",
        "    def ConvBlock(self, input_channels, output_channels, kernel_size=3, stride=1, padding = 1, maxpool = False):\n",
        "\n",
        "      if maxpool:\n",
        "        return Sequential(\n",
        "          Conv2d(input_channels, output_channels, kernel_size, stride, padding),\n",
        "          ReLU(inplace=True),\n",
        "          MaxPool2d(kernel_size=2, stride=2)\n",
        "          )\n",
        "      else:\n",
        "        return Sequential(\n",
        "          Conv2d(input_channels, output_channels, kernel_size, stride, padding),\n",
        "          ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def LinearBlock(self, input_channels, output_channels):\n",
        "\n",
        "      return Sequential(\n",
        "            Linear(input_channels, output_channels), \n",
        "            ReLU(inplace=True),\n",
        "            LayerNorm(output_channels)\n",
        "      )\n",
        "          \n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.sound_detector_model(x)\n",
        "        \n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[2] * sizes[3], sizes[1])\n",
        "        ##x = x.transpose(1, 2)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        \n",
        "        return x\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0NtgWDySfYh"
      },
      "source": [
        "## Load data and create data generators\n",
        "\n",
        "I merged test with train because of the data scarcity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7mL1_VeB5KJ"
      },
      "source": [
        "data_mode = [\"Train\", \"Val\", \"Test\"]\n",
        "\n",
        "for mode in data_mode:\n",
        "\n",
        "  i = 0\n",
        "  sp_path = \"Audio_Pad/\" + mode + \"/Speech\"\n",
        "  nonsp_path = \"Audio_Pad/\" + mode + \"/Non_Speech\"\n",
        "\n",
        "  sp_tensor = torch.empty((len(os.listdir(sp_path)),128, 157))\n",
        "  nonsp_tensor = torch.empty((len(os.listdir(nonsp_path)),128, 157))  \n",
        "\n",
        "  for song in os.listdir(sp_path):\n",
        "    sound, _ = torchaudio.load(sp_path + \"/\" + song, channels_first = False)\n",
        "    sound = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128, n_fft = 2048)(sound.flatten())\n",
        "    if mode != \"Val\":\n",
        "      sound = torchaudio.transforms.FrequencyMasking(freq_mask_param = 15)(sound)\n",
        "      sound = torchaudio.transforms.TimeMasking(time_mask_param = 35)(sound)\n",
        "\n",
        "\n",
        "    sp_tensor[i] = sound\n",
        "    i += 1\n",
        "\n",
        "  i = 0\n",
        "  for song in os.listdir(nonsp_path):\n",
        "    sound, _ = torchaudio.load(nonsp_path + \"/\" + song, channels_first = False)\n",
        "    sound = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128, n_fft = 2048)(sound.flatten())\n",
        "    if mode != \"Val\":\n",
        "      sound = torchaudio.transforms.FrequencyMasking(freq_mask_param = 15)(sound)\n",
        "      sound = torchaudio.transforms.TimeMasking(time_mask_param = 35)(sound)\n",
        "\n",
        "    nonsp_tensor[i] = sound\n",
        "    i += 1\n",
        "      \n",
        "  if mode == \"Train\":\n",
        "\n",
        "    train_data = torch.cat((sp_tensor, nonsp_tensor))\n",
        "    train_labels = torch.cat((torch.ones(len(os.listdir(sp_path))), torch.zeros(len(os.listdir(nonsp_path)))))\n",
        "\n",
        "    aug_train_data = torch.zeros((50,128,157))\n",
        "    aug_train_labels = torch.zeros((50))\n",
        "\n",
        "    train_data = torch.cat((train_data, aug_train_data))\n",
        "    train_labels = torch.cat((train_labels,aug_train_labels))\n",
        "\n",
        "    train_data = train_data[:,None, ...]\n",
        "\n",
        "  elif mode == \"Val\":\n",
        "\n",
        "    val_data = torch.cat((sp_tensor, nonsp_tensor))\n",
        "    val_labels = torch.cat((torch.ones(len(os.listdir(sp_path))), torch.zeros(len(os.listdir(nonsp_path)))))\n",
        "\n",
        "    val_data = val_data[:,None, ...]\n",
        "\n",
        "  else:\n",
        "\n",
        "    test_data = torch.cat((sp_tensor, nonsp_tensor))\n",
        "    test_labels = torch.cat((torch.ones(len(os.listdir(sp_path))), torch.zeros(len(os.listdir(nonsp_path))))) \n",
        "\n",
        "    test_data = test_data[:,None, ...]\n",
        "\n",
        "\n",
        "  sp_tensor = []\n",
        "  nonsp_tensor = []\n",
        "\n",
        "  del sp_tensor\n",
        "  del nonsp_tensor\n",
        "\n",
        "\n",
        "train_data = torch.cat((train_data, test_data))\n",
        "train_labels = torch.cat((train_labels, test_labels))\n",
        "\n",
        "training_set = CustomDataset(train_data, train_labels)\n",
        "training_generator = DataLoader(training_set, batch_size = 128, shuffle = True)\n",
        "\n",
        "val_set = CustomDataset(val_data, val_labels)\n",
        "val_generator = DataLoader(training_set, batch_size = 128, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9xlC7XrS-Nw"
      },
      "source": [
        "## Initialize model, loss, and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLeXcbAxQrsQ"
      },
      "source": [
        "model = SoundDetectorModel()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=0.0001)\n",
        "\n",
        "scheduler = lr_scheduler.OneCycleLR(optimizer,\n",
        "\tmax_lr=0.0004,\n",
        "\tsteps_per_epoch=int(len(training_generator)),\n",
        "\tepochs=50,\n",
        "\tanneal_strategy='linear')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl0a02UKTGG8"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukNlufxtkW2t"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_epochs = 50\n",
        "\n",
        "best_acc = 0.0\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "val_acc_history = []\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(1, n_epochs+1)):\n",
        "    \n",
        "    for phase in ['train', 'val']:\n",
        "        \n",
        "        if phase == 'train':\n",
        "            model.train()  \n",
        "        else:\n",
        "            model.eval()   \n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        " \n",
        "\n",
        "        for inputs, labels in training_generator:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                \n",
        "                _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()           \n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_loss = running_loss / len(training_generator.dataset)\n",
        "        epoch_acc = running_corrects.double() / len(training_generator.dataset)\n",
        "\n",
        "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "    \n",
        "        if phase == 'val' and epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        if phase == 'val':\n",
        "            val_acc_history.append(epoch_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uth7TZRvkW24"
      },
      "source": [
        "model.load_state_dict(best_model_wts)\n",
        "\n",
        "torch.save(best_model_wts, \"SoundDetector.pth\")\n",
        "\n",
        "torch.save(model,'SoundDetector.pt')"
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}